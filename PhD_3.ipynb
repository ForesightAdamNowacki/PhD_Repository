{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import skimage.morphology as smo\n",
    "import time\n",
    "from random import randrange\n",
    "import itertools\n",
    "\n",
    "import glob \n",
    "\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import cuda \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "os.chdir(\"D:/GitHub/PhD_Repository/Datasets/MPEG7_CE-Shape-1_Part_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_cv2(image, tytul = '', osie = False, opencv = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot image\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(osie):\n",
    "        plt.axis(\"off\") \n",
    "    if image.ndim == 2:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        if opencv:\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(image)\n",
    "    plt.title(tytul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_add_border(image, x , y, val, rel = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Adds a border to the input image\n",
    "    rel = True - x,y - size of the added border\n",
    "    rel = False - x,y - size of the image with added border\n",
    "    val - pixel value \n",
    "    \"\"\"\n",
    "    \n",
    "    (sx_org, sy_org) = image.shape\n",
    "    if rel:\n",
    "        sx = sx_org + 2*x\n",
    "        sy = sy_org + 2*y\n",
    "    else:\n",
    "        sx = x\n",
    "        sy = y\n",
    "    img = np.ones([sx, sy]) * val\n",
    "    [cx, cy] = ((np.array([sx, sy]) - np.array([sx_org, sy_org]))/2).astype('int')\n",
    "    img[cx:(cx + sx_org), cy:(cy + sy_org)] = image\n",
    "    return img\n",
    "\n",
    "# ---------- #\n",
    "# Use example:\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# print(obraz.shape)\n",
    "# obraz_ = image_add_border(obraz, x = 100, y = 100, val = 1, rel = True)\n",
    "# print(obraz_.shape)\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(obraz)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(obraz_)\n",
    "# plt.show()\n",
    "# ---------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structuring_elements(n, type = 'disk', initsize = 1, step = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    generate a list of n structuring elements\n",
    "    type = 'disk', 'square', 'diamond'\n",
    "    \"\"\"\n",
    "    \n",
    "    selist = []\n",
    "    sesize = initsize\n",
    "    \n",
    "    for i in range(n):\n",
    "        if (type == 'disk'):\n",
    "            se = smo.disk(sesize)\n",
    "        if (type == 'square'):\n",
    "            se = smo.square(2 * sesize + 1)\n",
    "        if (type == 'diamond'):\n",
    "            se = smo.diamond(sesize)       \n",
    "        \n",
    "        selist.append(se)\n",
    "        sesize += step\n",
    "        \n",
    "    return selist\n",
    "\n",
    "# ---------- #\n",
    "# Use examples:\n",
    "# get_structuring_elements(3, type = \"square\")\n",
    "# get_structuring_elements(3, type = \"disk\")\n",
    "# get_structuring_elements(3, type = \"diamond\")\n",
    "# ---------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_stack(input_image,\n",
    "                        structuring_elements_depth,\n",
    "                        transormation_type = 'cv_oc',\n",
    "                        structuring_elements_type = 'disk',\n",
    "                        structuring_elements_initsize = 1,\n",
    "                        structuring_elements_step = 1,\n",
    "                        addborder = True):\n",
    "    \"\"\"\n",
    "    produce a stack of results of the morphological dual operators\n",
    "    input_image - imput image (binary or graylevel 2D image)\n",
    "    structuring_elements_depth - list of two values - numers of up-stack and down-stack images\n",
    "    transormation_type - type of operations erosion/dilation <-> opening/closing; skimage binary <-> skimage graytone <-> opencv\n",
    "    structuring_elements_type = structuring element type ('disk', 'square', 'diamond')\n",
    "    structuring_elements_initsize = initial size of the structuring element \n",
    "    structuring_elements_step = increment of the structuring element size\n",
    "    addborder = True if the external boundary is added, = False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    max_up = structuring_elements_depth[0] # number of up-stack images (higher indeces, dilation/opening)\n",
    "    max_down = structuring_elements_depth[1] # number of down-stack images (kower indeces, erosion/closing)\n",
    "    max_updown = max(max_up, max_down)\n",
    "    \n",
    "    structuring_elements_list = get_structuring_elements(n = max_updown,\n",
    "                                                         type = structuring_elements_type, \n",
    "                                                         initsize = structuring_elements_initsize,\n",
    "                                                         step = structuring_elements_step)\n",
    "    if addborder:\n",
    "        image = image_add_border(image = input_image,\n",
    "                                 x = max_updown,\n",
    "                                 y = max_updown, \n",
    "                                 val = 0, \n",
    "                                 rel = True)\n",
    "    else:\n",
    "        image = input_image\n",
    "    \n",
    "    image_out = np.zeros([image.shape[0], image.shape[1], max_up + max_down + 1])    \n",
    "    count = 0\n",
    "\n",
    "    if transormation_type == 'b_ed': # binary erosion/dilation - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.binary_erosion\n",
    "        operator_up = smo.binary_dilation\n",
    "    elif transormation_type == 'b_oc': # binary opening/closing - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.binary_opening\n",
    "        operator_up = smo.binary_closing      \n",
    "    elif transormation_type == 'ed': # erosion/dilation - scikit.image     \n",
    "        opencv = False\n",
    "        operator_down = smo.erosion\n",
    "        operator_up = smo.dilation\n",
    "    elif transormation_type == 'oc': # opening/closing - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.opening\n",
    "        operator_up = smo.closing      \n",
    "    elif transormation_type == 'cv_ed': # erosion/dilation - openCV \n",
    "        opencv = True\n",
    "        operator_down = cv2.MORPH_ERODE\n",
    "        operator_up = cv2.MORPH_DILATE\n",
    "    else: # transormation_type == 'cv_oc': # opening/closing - openCV \n",
    "        opencv = True\n",
    "        operator_down = cv2.MORPH_OPEN\n",
    "        operator_up = cv2.MORPH_CLOSE     \n",
    "    \n",
    "    if opencv:  # opencv version \n",
    "        for i in range(max_down):\n",
    "            image_out[:,:,count] = cv2.morphologyEx(image,\n",
    "                                                    operator_down,\n",
    "                                                    structuring_elements_list[max_down - i - 1]); count += 1\n",
    "        image_out[:,:,count] = image; count += 1\n",
    "        for i in range(max_up):\n",
    "            image_out[:,:,count] = cv2.morphologyEx(image,\n",
    "                                                    operator_up, \n",
    "                                                    structuring_elements_list[i]); count += 1\n",
    "        \n",
    "    else:   # scikit image version\n",
    "        for i in range(max_down):\n",
    "            operator_down(image,\n",
    "                          selem = structuring_elements_list[max_down - i - 1],\n",
    "                          out = image_out[:,:,count]); count += 1\n",
    "        image_out[:,:,count] = image; count += 1\n",
    "        for i in range(max_up):\n",
    "            operator_up(image,\n",
    "                        selem = structuring_elements_list[i], \n",
    "                        out = image_out[:,:,count]); count += 1   \n",
    "        \n",
    "    return image_out\n",
    "\n",
    "# ---------- #\n",
    "# Use examples:\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# obraz_ = morphological_stack(input_image = obraz,\n",
    "#                              structuring_elements_depth = [2, 2])\n",
    "# obraz_.shape\n",
    "# ---------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_morphological_stack(input_stack, normalize = True):\n",
    "\n",
    "    \"\"\"\n",
    "    flat morphological stack\n",
    "    normalize - convert result for pixel values between 0-1\n",
    "    \"\"\"\n",
    "    \n",
    "    max_up_down = input_stack.shape[2]\n",
    "    max_value = input_stack.max()\n",
    "    image_out = np.sum(input_stack/max_value, axis = 2)\n",
    "    if normalize:\n",
    "            image_out /= max_up_down\n",
    "    \n",
    "    return image_out\n",
    "\n",
    "# ---------- #\n",
    "# Use examples:\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# obraz_ = morphological_stack(input_image = obraz,\n",
    "#                              structuring_elements_depth = [25, 25],\n",
    "#                             transormation_type = \"cv_ed\")\n",
    "# obraz__ = flat_morphological_stack(input_stack = obraz_)\n",
    "# plt.imshow(obraz__)\n",
    "\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# obraz_ = morphological_stack(input_image = obraz,\n",
    "#                              structuring_elements_depth = [25, 25],\n",
    "#                             transormation_type = \"cv_oc\")\n",
    "# obraz__ = flat_morphological_stack(input_stack = obraz_)\n",
    "# plt.imshow(obraz__)\n",
    "# ---------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_stack_with_flat(input_image,\n",
    "                                  structuring_elements_depth,\n",
    "                                  transormation_type = 'cv_oc',\n",
    "                                  structuring_elements_type = 'disk',\n",
    "                                  structuring_elements_initsize = 1,\n",
    "                                  structuring_elements_step = 1,\n",
    "                                  addborder = True,\n",
    "                                  normalize_flat = True):\n",
    "    \"\"\"\n",
    "    produce a stack of results of the morphological dual operators\n",
    "    input_image - imput image (binary or graylevel 2D image)\n",
    "    structuring_elements_depth - list of two values - numers of up-stack and down-stack images\n",
    "    transormation_type - type of operations erosion/dilation <-> opening/closing; skimage binary <-> skimage graytone <-> opencv\n",
    "    structuring_elements_type = structuring element type ('disk', 'square', 'diamond')\n",
    "    structuring_elements_initsize = initial size of the structuring element \n",
    "    structuring_elements_step = increment of the structuring element size\n",
    "    addborder = True if the external boundary is added, = False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    max_up = structuring_elements_depth[0] # number of up-stack images (higher indeces, dilation/opening)\n",
    "    max_down = structuring_elements_depth[1] # number of down-stack images (kower indeces, erosion/closing)\n",
    "    max_updown = max(max_up, max_down)\n",
    "    \n",
    "    structuring_elements_list = get_structuring_elements(n = max_updown,\n",
    "                                                         type = structuring_elements_type, \n",
    "                                                         initsize = structuring_elements_initsize,\n",
    "                                                         step = structuring_elements_step)\n",
    "    if addborder:\n",
    "        image = image_add_border(image = input_image,\n",
    "                                 x = max_updown,\n",
    "                                 y = max_updown, \n",
    "                                 val = 0, \n",
    "                                 rel = True)\n",
    "    else:\n",
    "        image = input_image\n",
    "    \n",
    "    image_out = np.zeros([image.shape[0], image.shape[1], max_up + max_down + 1])    \n",
    "    count = 0\n",
    "\n",
    "    if transormation_type == 'b_ed': # binary erosion/dilation - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.binary_erosion\n",
    "        operator_up = smo.binary_dilation\n",
    "    elif transormation_type == 'b_oc': # binary opening/closing - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.binary_opening\n",
    "        operator_up = smo.binary_closing      \n",
    "    elif transormation_type == 'ed': # erosion/dilation - scikit.image     \n",
    "        opencv = False\n",
    "        operator_down = smo.erosion\n",
    "        operator_up = smo.dilation\n",
    "    elif transormation_type == 'oc': # opening/closing - scikit.image\n",
    "        opencv = False\n",
    "        operator_down = smo.opening\n",
    "        operator_up = smo.closing      \n",
    "    elif transormation_type == 'cv_ed': # erosion/dilation - openCV \n",
    "        opencv = True\n",
    "        operator_down = cv2.MORPH_ERODE\n",
    "        operator_up = cv2.MORPH_DILATE\n",
    "    else: # transormation_type == 'cv_oc': # opening/closing - openCV \n",
    "        opencv = True\n",
    "        operator_down = cv2.MORPH_OPEN\n",
    "        operator_up = cv2.MORPH_CLOSE     \n",
    "    \n",
    "    if opencv:  # opencv version \n",
    "        for i in range(max_down):\n",
    "            image_out[:,:,count] = cv2.morphologyEx(image,\n",
    "                                                    operator_down,\n",
    "                                                    structuring_elements_list[max_down - i - 1]); count += 1\n",
    "        image_out[:,:,count] = image; count += 1\n",
    "        for i in range(max_up):\n",
    "            image_out[:,:,count] = cv2.morphologyEx(image,\n",
    "                                                    operator_up, \n",
    "                                                    structuring_elements_list[i]); count += 1\n",
    "        \n",
    "    else:   # scikit image version\n",
    "        for i in range(max_down):\n",
    "            operator_down(image,\n",
    "                          selem = structuring_elements_list[max_down - i - 1],\n",
    "                          out = image_out[:,:,count]); count += 1\n",
    "        image_out[:,:,count] = image; count += 1\n",
    "        for i in range(max_up):\n",
    "            operator_up(image,\n",
    "                        selem = structuring_elements_list[i], \n",
    "                        out = image_out[:,:,count]); count += 1   \n",
    "            \n",
    "    image_out = flat_morphological_stack(input_stack = image_out, normalize = normalize_flat)\n",
    "        \n",
    "    return image_out\n",
    "\n",
    "# ---------- #\n",
    "# Use examples:\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# obraz_ = morphological_stack_with_flat(input_image = obraz,\n",
    "#                                        transormation_type = \"cv_oc\",\n",
    "#                                        structuring_elements_depth = [10, 10])\n",
    "# obraz_.shape\n",
    "# plt.imshow(obraz_)\n",
    "\n",
    "# obraz = imread(\"camel-19.gif\")\n",
    "# if len(obraz.shape) == 3:\n",
    "#     obraz = obraz[:,:,1]\n",
    "# obraz_ = morphological_stack_with_flat(input_image = obraz,\n",
    "#                                        transormation_type = \"cv_ed\",\n",
    "#                                        structuring_elements_depth = [25, 25])\n",
    "# obraz_.shape\n",
    "# plt.imshow(obraz_)\n",
    "# ---------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(image_size, channels, start_neurons, dense_neurons, classes, model_name = \"model_1\"):\n",
    "     \n",
    "    input_tensor = tf.keras.layers.Input(shape = [image_size, image_size, channels])\n",
    "\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters = start_neurons * 1, \n",
    "                                    kernel_size = (3, 3),\n",
    "                                    strides = (1, 1), \n",
    "                                    activation = tf.keras.activations.relu,\n",
    "                                    padding = \"same\")(input_tensor)\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters = start_neurons * 1, \n",
    "                                    kernel_size = (3, 3),\n",
    "                                    strides = (1, 1), \n",
    "                                    activation = tf.keras.activations.relu,\n",
    "                                    padding = \"same\")(conv_1)\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters = start_neurons * 1, \n",
    "                                    kernel_size = (3, 3),\n",
    "                                    strides = (1, 1), \n",
    "                                    activation = tf.keras.activations.relu,\n",
    "                                    padding = \"same\")(conv_2)\n",
    "    pool_1 = tf.keras.layers.MaxPool2D(pool_size = (2, 2),\n",
    "                                       strides = (2, 2))(conv_3)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(pool_1)\n",
    "    dense = tf.keras.layers.Dense(units = dense_neurons, activation = tf.keras.activations.relu)(flatten)\n",
    "    output_tensor = tf.keras.layers.Dense(units = classes, activation = tf.keras.activations.softmax)(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = input_tensor, \n",
    "                                  outputs = output_tensor, \n",
    "                                  name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Step images_1:\n",
      "2. Step images_2:\n",
      "3. Step images_3:\n",
      "4. Step images_4:\n",
      "5. Step images_5:\n",
      "6. Step images_6:\n",
      "7. Step images_7:\n",
      "8. Step images_8:\n",
      "(1400, 128, 128, 1)\n",
      "(3360, 128, 128, 1)\n",
      "(3360, 70)\n",
      "(1120, 128, 128, 1)\n",
      "(1120, 70)\n",
      "(1120, 128, 128, 1)\n",
      "(1120, 70)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               33554944  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 70)                35910     \n",
      "=================================================================\n",
      "Total params: 33,595,654\n",
      "Trainable params: 33,595,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Augmentation mode on\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 53.0 steps, validate for 18.0 steps\n",
      "Epoch 1/100\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 3.5016 - accuracy: 0.1357 - val_loss: 2.8029 - val_accuracy: 0.2589\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 2.5338 - accuracy: 0.2935 - val_loss: 2.1916 - val_accuracy: 0.3875\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 2.1292 - accuracy: 0.3860 - val_loss: 1.9510 - val_accuracy: 0.4616\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 1.8722 - accuracy: 0.4497 - val_loss: 1.7292 - val_accuracy: 0.5607\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 3s 56ms/step - loss: 1.6493 - accuracy: 0.5033 - val_loss: 1.6211 - val_accuracy: 0.5875\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 3s 56ms/step - loss: 1.5386 - accuracy: 0.5363 - val_loss: 1.4865 - val_accuracy: 0.6018\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 1.3483 - accuracy: 0.5887 - val_loss: 1.5558 - val_accuracy: 0.5839\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 1.2393 - accuracy: 0.6176 - val_loss: 1.3184 - val_accuracy: 0.6821\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 3s 56ms/step - loss: 1.1102 - accuracy: 0.6643 - val_loss: 1.2574 - val_accuracy: 0.6902\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 1.0005 - accuracy: 0.6866 - val_loss: 1.2587 - val_accuracy: 0.7027\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.9134 - accuracy: 0.7128 - val_loss: 1.2351 - val_accuracy: 0.7116\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.8697 - accuracy: 0.7259 - val_loss: 1.1664 - val_accuracy: 0.7196\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.8141 - accuracy: 0.7440 - val_loss: 1.1594 - val_accuracy: 0.7411\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.7257 - accuracy: 0.7708 - val_loss: 1.1007 - val_accuracy: 0.7688\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.6934 - accuracy: 0.7810 - val_loss: 1.1523 - val_accuracy: 0.7902\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.6754 - accuracy: 0.7845 - val_loss: 1.1433 - val_accuracy: 0.7482\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.6278 - accuracy: 0.7952 - val_loss: 1.1170 - val_accuracy: 0.7688\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.6053 - accuracy: 0.8033 - val_loss: 1.2222 - val_accuracy: 0.7393\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.5569 - accuracy: 0.8199 - val_loss: 1.0901 - val_accuracy: 0.8036\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.5264 - accuracy: 0.8274 - val_loss: 1.1023 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.4948 - accuracy: 0.8443 - val_loss: 1.1554 - val_accuracy: 0.7812\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.5019 - accuracy: 0.8396 - val_loss: 1.0856 - val_accuracy: 0.8071\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.4719 - accuracy: 0.8452 - val_loss: 1.2359 - val_accuracy: 0.7625\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.4734 - accuracy: 0.8470 - val_loss: 1.1246 - val_accuracy: 0.8134\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.4102 - accuracy: 0.8583 - val_loss: 1.2033 - val_accuracy: 0.7839\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.3711 - accuracy: 0.8798 - val_loss: 1.1863 - val_accuracy: 0.8080\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3893 - accuracy: 0.8756 - val_loss: 1.3640 - val_accuracy: 0.7679\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3747 - accuracy: 0.8783 - val_loss: 1.2807 - val_accuracy: 0.7875\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 3s 56ms/step - loss: 0.3813 - accuracy: 0.8744 - val_loss: 1.1639 - val_accuracy: 0.7911\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3788 - accuracy: 0.8771 - val_loss: 1.0730 - val_accuracy: 0.8196\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3249 - accuracy: 0.8997 - val_loss: 1.1200 - val_accuracy: 0.8196\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.3225 - accuracy: 0.8926 - val_loss: 1.4373 - val_accuracy: 0.7670\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.3170 - accuracy: 0.8940 - val_loss: 1.2524 - val_accuracy: 0.7893\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.3098 - accuracy: 0.9003 - val_loss: 1.4132 - val_accuracy: 0.7563\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.3454 - accuracy: 0.8833 - val_loss: 1.1484 - val_accuracy: 0.8107\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3172 - accuracy: 0.8970 - val_loss: 1.1438 - val_accuracy: 0.8286\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3214 - accuracy: 0.8964 - val_loss: 1.1157 - val_accuracy: 0.8366\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.3090 - accuracy: 0.8973 - val_loss: 1.2358 - val_accuracy: 0.7946\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.2909 - accuracy: 0.9092 - val_loss: 1.1446 - val_accuracy: 0.8098\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.2678 - accuracy: 0.9030 - val_loss: 1.1963 - val_accuracy: 0.8071\n",
      "        loss  accuracy  val_loss  val_accuracy  model  epoch\n",
      "0   3.501950  0.135714  2.802915      0.258929  model      1\n",
      "1   2.537367  0.293452  2.191558      0.387500  model      2\n",
      "2   2.129190  0.386012  1.951022      0.461607  model      3\n",
      "3   1.872228  0.449702  1.729226      0.560714  model      4\n",
      "4   1.648775  0.503274  1.621056      0.587500  model      5\n",
      "5   1.540847  0.536310  1.486495      0.601786  model      6\n",
      "6   1.350146  0.588690  1.555809      0.583929  model      7\n",
      "7   1.239079  0.617560  1.318370      0.682143  model      8\n",
      "8   1.111843  0.664286  1.257403      0.690179  model      9\n",
      "9   1.002182  0.686607  1.258653      0.702679  model     10\n",
      "10  0.913974  0.712798  1.235121      0.711607  model     11\n",
      "11  0.867799  0.725893  1.166373      0.719643  model     12\n",
      "12  0.812595  0.744048  1.159393      0.741071  model     13\n",
      "13  0.726277  0.770833  1.100659      0.768750  model     14\n",
      "14  0.694719  0.780952  1.152326      0.790179  model     15\n",
      "15  0.676656  0.784524  1.143334      0.748214  model     16\n",
      "16  0.628014  0.795238  1.117029      0.768750  model     17\n",
      "17  0.607692  0.803274  1.222243      0.739286  model     18\n",
      "18  0.555074  0.819940  1.090149      0.803571  model     19\n",
      "19  0.525259  0.827381  1.102345      0.800000  model     20\n",
      "20  0.494828  0.844345  1.155445      0.781250  model     21\n",
      "21  0.502674  0.839583  1.085605      0.807143  model     22\n",
      "22  0.472414  0.845238  1.235893      0.762500  model     23\n",
      "23  0.475317  0.847024  1.124646      0.813393  model     24\n",
      "24  0.410572  0.858333  1.203285      0.783929  model     25\n",
      "25  0.371678  0.879762  1.186334      0.808036  model     26\n",
      "26  0.391618  0.875595  1.364047      0.767857  model     27\n",
      "27  0.375774  0.878274  1.280672      0.787500  model     28\n",
      "28  0.382303  0.874405  1.163947      0.791071  model     29\n",
      "29  0.378594  0.877083  1.073001      0.819643  model     30\n",
      "30  0.324738  0.899702  1.120027      0.819643  model     31\n",
      "31  0.322750  0.892560  1.437317      0.766964  model     32\n",
      "32  0.318617  0.894048  1.252411      0.789286  model     33\n",
      "33  0.309168  0.900298  1.413218      0.756250  model     34\n",
      "34  0.347060  0.883333  1.148356      0.810714  model     35\n",
      "35  0.318525  0.897024  1.143783      0.828571  model     36\n",
      "36  0.321776  0.896429  1.115691      0.836607  model     37\n",
      "37  0.305972  0.897321  1.235788      0.794643  model     38\n",
      "38  0.290196  0.909226  1.144625      0.809821  model     39\n",
      "39  0.267966  0.902976  1.196251      0.807143  model     40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360/3360 [==============================] - 1s 233us/sample - loss: 0.1768 - accuracy: 0.9417\n",
      "1120/1120 [==============================] - 0s 219us/sample - loss: 1.2152 - accuracy: 0.8071\n",
      "1120/1120 [==============================] - 0s 217us/sample - loss: 0.9459 - accuracy: 0.8143\n",
      "Train accuracy: 0.94166666\n",
      "Validation accuracy: 0.80714285\n",
      "Test accuracy: 0.8142857\n",
      "      Dataset  Accuracy Model_Name\n",
      "0       train  0.941667      model\n",
      "1  validation  0.807143      model\n",
      "2        test  0.814286      model\n"
     ]
    }
   ],
   "source": [
    "def morphological_transformation_pipe_improved(results_directory = \"D:/GitHub/PhD_Repository/Results_2/\",\n",
    "                                               model_name = \"model\",\n",
    "                                               data_directory = \"D:/GitHub/PhD_Repository/Datasets/MPEG7_CE-Shape-1_Part_B\",\n",
    "                                               image_size_ = 128,\n",
    "                                               morphological_transformation_mode = 2,\n",
    "                                               structuring_elements_depth_ = [5, 5],\n",
    "                                               transormation_type_ = \"cv_ed\",\n",
    "                                               structuring_elements_type_ = \"disk\",\n",
    "                                               structuring_elements_initsize_ = 1,\n",
    "                                               structuring_elements_step_ = 1,\n",
    "                                               addborder_ = True,\n",
    "                                               train_flip_augmentation = True,\n",
    "                                               validation_flip_augmentation = True,\n",
    "                                               test_flip_augmentation = True,\n",
    "                                               augmentation = True,\n",
    "                                               channels_ = 1,\n",
    "                                               start_neurons_ = 16,\n",
    "                                               dense_neurons_ = 512,\n",
    "                                               batch_size = 64,\n",
    "                                               epochs = 100,\n",
    "                                               early_stopping = 10,\n",
    "                                               shuffle = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    results_directory - directory where results are saved\n",
    "    model_name - string model name\n",
    "    data_directory - directory with input data\n",
    "    image_size_ - size of input (width, height) to CNN model\n",
    "    morphological_transformation_mode - 0 - without any morphological transformations\n",
    "                                      - 1 - with morphological transformations and stack of mola is stored in memory - require a lot od RAM for bigger stacks - memory insufficient method\n",
    "                                      - 2 - with morphological transformations and stack is normalized and flattened immediately - memory efficient method\n",
    "    structuring_elements_depth_ - MoLa stack size\n",
    "    transormation_type_ - e.g. erosion/dillataion or opening/closing\n",
    "    structuring_elements_type_ - e.g. disk, square, diamond\n",
    "    structuring_elements_initsize - structuring_elements_initsize_\n",
    "    structuring_elements_step_ - structuring_elements_step_\n",
    "    addborder_ - addborder\n",
    "    train_flip_augmentation - data generation/augmentation using flips: 90, 180, 270 degrees on train data\n",
    "    validation_flip_augmentation - data generation/augmentation using flips: 90, 180, 270 degrees on validation data\n",
    "    test_flip_augmentation - data generation/augmentation using flips: 90, 180, 270 degrees on test data\n",
    "    augmentation - train data augmentation during training\n",
    "    channels_ - depth in CNN model - greyscale images -> 1, RGB -> 3\n",
    "    start_neurons_ - filters in Conv layers\n",
    "    dense_neurons_ - neurond in dense layer between flattening and output layer\n",
    "    batch_size - number of images in one batch\n",
    "    epochs - epochs\n",
    "    early_stopping - early stopping\n",
    "    shuffle - randomly shuffle data during training \n",
    "    \"\"\"\n",
    "    \n",
    "    os.chdir(data_directory)\n",
    "    data = pd.DataFrame({\"Path\" : [os.path.join(os.getcwd(), i) for i in os.listdir()],\n",
    "                         \"Filename\" : [i for i in os.listdir()],\n",
    "                         \"Label\" : [i[0:i.find(\"-\")] for i in os.listdir()]}); data\n",
    "    data = data.merge(pd.DataFrame({\"Label\" : data[\"Label\"].unique().tolist(),\n",
    "                                    \"Numeric_Label\" : np.arange(len(data[\"Label\"].unique()))}), \n",
    "                      how = \"left\",\n",
    "                      on = \"Label\")\n",
    "    classes = len(data[\"Label\"].unique())\n",
    "\n",
    "    if morphological_transformation_mode == 1:\n",
    "        print(\"1. Step images_1:\")\n",
    "        images_1 = [imread(i) for i in data[\"Path\"].tolist()]\n",
    "        print(\"2. Step images_2:\")\n",
    "        images_2 = [i[:,:,0] if len(i.shape) == 3 else i for i in images_1]\n",
    "        del images_1\n",
    "        print(\"3. Step images_3:\")\n",
    "        images_3 = [morphological_stack(input_image = i,\n",
    "                                      structuring_elements_depth = structuring_elements_depth_,\n",
    "                                      transormation_type = transormation_type_,\n",
    "                                      structuring_elements_type = structuring_elements_type_,\n",
    "                                      structuring_elements_initsize = structuring_elements_initsize_,\n",
    "                                      structuring_elements_step = structuring_elements_step_,\n",
    "                                      addborder = addborder_) for i in images_2]\n",
    "        del images_2\n",
    "        print(\"4. Step images_4:\")\n",
    "        images_4 = [flat_morphological_stack(i) for i in images_3]\n",
    "        del images_3\n",
    "        print(\"5. Step images_5:\")\n",
    "        images_5 = [cv2.resize(i, (image_size_, image_size_)) for i in images_4]\n",
    "        del images_4\n",
    "        print(\"6. Step images_6:\")\n",
    "        images_6 = [np.expand_dims(i, -1) for i in images_5]\n",
    "        del images_5\n",
    "        print(\"7. Step images_7:\")\n",
    "        images_7 = [np.expand_dims(i, 0) for i in images_6]\n",
    "        del images_6\n",
    "        print(\"8. Step images_8:\")\n",
    "        images_8 = np.concatenate(np.array(images_7), axis = 0)\n",
    "        del images_7\n",
    "\n",
    "    elif morphological_transformation_mode == 2:\n",
    "\n",
    "        print(\"1. Step images_1:\")\n",
    "        images_1 = [imread(i) for i in data[\"Path\"].tolist()]\n",
    "        print(\"2. Step images_2:\")\n",
    "        images_2 = [i[:,:,0] if len(i.shape) == 3 else i for i in images_1]\n",
    "        del images_1\n",
    "        print(\"3. Step images_3:\")\n",
    "        images_3 = [morphological_stack_with_flat(input_image = i,\n",
    "                                                  structuring_elements_depth = structuring_elements_depth_,\n",
    "                                                  transormation_type = transormation_type_,\n",
    "                                                  structuring_elements_type = structuring_elements_type_,\n",
    "                                                  structuring_elements_initsize = structuring_elements_initsize_,\n",
    "                                                  structuring_elements_step = structuring_elements_step_,\n",
    "                                                  addborder = addborder_) for i in images_2]\n",
    "        del images_2\n",
    "        print(\"4. Step images_4:\")\n",
    "        images_4 = images_3\n",
    "        del images_3\n",
    "        print(\"5. Step images_5:\")\n",
    "        images_5 = [cv2.resize(i, (image_size_, image_size_)) for i in images_4]\n",
    "        del images_4\n",
    "        print(\"6. Step images_6:\")\n",
    "        images_6 = [np.expand_dims(i, -1) for i in images_5]\n",
    "        del images_5\n",
    "        print(\"7. Step images_7:\")\n",
    "        images_7 = [np.expand_dims(i, 0) for i in images_6]\n",
    "        del images_6\n",
    "        print(\"8. Step images_8:\")\n",
    "        images_8 = np.concatenate(np.array(images_7), axis = 0)\n",
    "        del images_7\n",
    "\n",
    "    else: # morphological_transformation_mode == 0:  \n",
    "        print(\"1. Step images_1:\")\n",
    "        images_1 = [imread(i) for i in data[\"Path\"].tolist()]\n",
    "        print(\"2. Step images_2:\")\n",
    "        images_2 = [i[:,:,0] if len(i.shape) == 3 else i for i in images_1]\n",
    "        del images_1\n",
    "        print(\"3. Step images_3:\")\n",
    "        images_3 = images_2\n",
    "        del images_2\n",
    "        print(\"4. Step images_4:\")\n",
    "        images_4 = images_3\n",
    "        del images_3\n",
    "        print(\"5. Step images_5:\")\n",
    "        images_5 = [cv2.resize(i, (image_size_, image_size_)) for i in images_4]\n",
    "        del images_4\n",
    "        print(\"6. Step images_6:\")\n",
    "        images_6 = [np.expand_dims(i, -1) for i in images_5]\n",
    "        del images_5\n",
    "        print(\"7. Step images_7:\")\n",
    "        images_7 = [np.expand_dims(i, 0) for i in images_6]\n",
    "        del images_6\n",
    "        print(\"8. Step images_8:\")\n",
    "        images_8 = np.concatenate(np.array(images_7), axis = 0)\n",
    "        del images_7\n",
    "\n",
    "    print(np.array(images_8).shape)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    X = np.arange(data.shape[0])\n",
    "    y = np.arange(data.shape[0])\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.25, random_state = 42)\n",
    "\n",
    "    X_train_ = images_8[X_train,:,:,:]\n",
    "    y_train_ = np.array(data[\"Numeric_Label\"])[y_train]\n",
    "    X_valid_ = images_8[X_valid,:,:,:]\n",
    "    y_valid_ = np.array(data[\"Numeric_Label\"])[y_valid]\n",
    "    X_test_ = images_8[X_test,:,:,:]\n",
    "    y_test_ = np.array(data[\"Numeric_Label\"])[y_test]    \n",
    "\n",
    "    if train_flip_augmentation:\n",
    "        X_train_0 = [np.flip(i, axis = 0) for i in X_train_]\n",
    "        X_train_1 = [np.flip(i, axis = 1) for i in X_train_]\n",
    "        X_train_2 = [np.flip(np.flip(i, axis = 0), axis = 1) for i in X_train_]\n",
    "        X_train_ = np.concatenate((X_train_, X_train_0, X_train_1, X_train_2), axis = 0)\n",
    "        y_train_ = np.concatenate((y_train_, y_train_, y_train_, y_train_), axis = 0)\n",
    "\n",
    "    if validation_flip_augmentation:\n",
    "        X_valid_0 = [np.flip(i, axis = 0) for i in X_valid_]\n",
    "        X_valid_1 = [np.flip(i, axis = 1) for i in X_valid_]\n",
    "        X_valid_2 = [np.flip(np.flip(i, axis = 0), axis = 1) for i in X_valid_]\n",
    "        X_valid_ = np.concatenate((X_valid_, X_valid_0, X_valid_1, X_valid_2), axis = 0)\n",
    "        y_valid_ = np.concatenate((y_valid_, y_valid_, y_valid_, y_valid_), axis = 0)\n",
    "\n",
    "    if test_flip_augmentation:\n",
    "        X_test_0 = [np.flip(i, axis = 0) for i in X_test_]\n",
    "        X_test_1 = [np.flip(i, axis = 1) for i in X_test_]\n",
    "        X_test_2 = [np.flip(np.flip(i, axis = 0), axis = 1) for i in X_test_]\n",
    "        X_test_ = np.concatenate((X_test_, X_test_0, X_test_1, X_test_2), axis = 0)\n",
    "        y_test_ = np.concatenate((y_test_, y_test_, y_test_, y_test_), axis = 0)\n",
    "\n",
    "    y_train_ = tf.keras.utils.to_categorical(y_train_, classes)\n",
    "    y_valid_ = tf.keras.utils.to_categorical(y_valid_, classes)\n",
    "    y_test_ = tf.keras.utils.to_categorical(y_test_, classes)\n",
    "\n",
    "    print(X_train_.shape)\n",
    "    print(y_train_.shape)\n",
    "    print(X_valid_.shape)\n",
    "    print(y_valid_.shape)\n",
    "    print(X_test_.shape)\n",
    "    print(y_test_.shape)\n",
    "\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range = 90,\n",
    "        width_shift_range = np.ceil(0.1 * image_size_), \n",
    "        height_shift_range = np.ceil(0.1 * image_size_),\n",
    "        horizontal_flip = True, \n",
    "        vertical_flip = True,\n",
    "        fill_mode = 'nearest')\n",
    "\n",
    "    test_validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range = 0,\n",
    "        width_shift_range = 0, \n",
    "        height_shift_range = 0,\n",
    "        horizontal_flip = False, \n",
    "        vertical_flip = False,\n",
    "        fill_mode = 'nearest')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(patience = early_stopping, monitor = 'val_loss')]\n",
    "\n",
    "    model = build_model(image_size = image_size_,\n",
    "                        channels = channels_, \n",
    "                        start_neurons = start_neurons_,\n",
    "                        dense_neurons = dense_neurons_, \n",
    "                        classes = classes)\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                  loss = tf.keras.losses.categorical_crossentropy,\n",
    "                  metrics = [\"accuracy\"])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    if augmentation:\n",
    "        print(\"Augmentation mode on\")\n",
    "        train_generator = train_datagen.flow(X_train_, y_train_, batch_size = batch_size)\n",
    "    else:\n",
    "        print(\"Augmentation mode off\")\n",
    "        train_generator = test_validation_datagen.flow(X_train_, y_train_, batch_size = batch_size)\n",
    "            \n",
    "    validation_generator = test_validation_datagen.flow(X_valid_, y_valid_, batch_size = batch_size)      \n",
    "    \n",
    "    model_results = model.fit(train_generator,\n",
    "                              validation_data = validation_generator,\n",
    "                              steps_per_epoch = np.ceil(train_generator.n / batch_size),\n",
    "                              validation_steps = np.ceil(validation_generator.n / batch_size),\n",
    "                              epochs = epochs,\n",
    "                              shuffle = shuffle,\n",
    "                              callbacks = callbacks)\n",
    "\n",
    "    model_results_pd = pd.DataFrame(model_results.history)\n",
    "    model_results_pd[\"model\"] = model_name\n",
    "    model_results_pd[\"epoch\"] = np.array(model_results.epoch) + 1\n",
    "    model_results_pd.to_csv(results_directory + model_name + \"_history.csv\")\n",
    "    print(model_results_pd)\n",
    "\n",
    "    model_train_accuracy = model.evaluate(X_train_, y_train_)[1]\n",
    "    model_validation_accuracy = model.evaluate(X_valid_, y_valid_)[1]\n",
    "    model_test_accuracy = model.evaluate(X_test_, y_test_)[1]\n",
    "    del model\n",
    "\n",
    "    print(\"Train accuracy:\", model_train_accuracy)\n",
    "    print(\"Validation accuracy:\", model_validation_accuracy)\n",
    "    print(\"Test accuracy:\", model_test_accuracy)\n",
    "\n",
    "    evaluation_results = pd.DataFrame({\"Dataset\" : [\"train\", \"validation\", \"test\"],\n",
    "                                        \"Accuracy\" : [model_train_accuracy, model_validation_accuracy, model_test_accuracy],\n",
    "                                        \"Model_Name\" : [model_name] * 3})\n",
    "    evaluation_results.to_csv(results_directory + model_name + \"_evaluation.csv\")\n",
    "    print(evaluation_results)\n",
    "    \n",
    "morphological_transformation_pipe_improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_ML_2",
   "language": "python",
   "name": "gpu_ml_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
